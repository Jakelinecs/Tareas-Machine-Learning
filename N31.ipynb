{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVf+kE0UUyAVMWjRJixDbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jakelinecs/Tareas-Machine-Learning/blob/main/N31.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7THsFFjpbg6",
        "outputId": "07cc4b89-2f69-4524-b2f4-41bbf174686a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-04 20:19:16--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  19.2MB/s    in 6.1s    \n",
            "\n",
            "2025-10-04 20:19:23 (13.1 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n",
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ]
        }
      ],
      "source": [
        "# Download IMDB to the current folder\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# Extract\n",
        "!tar zxf aclImdb_v1.tar.gz\n",
        "# Delete aclImdb/train/unsup as it has no labels\n",
        "!rm -rf aclImdb/train/unsup\n",
        "# Display IMDB dataset description\n",
        "!cat aclImdb/README"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_files\n",
        "\n",
        "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
        "x_train, y_train = train_review.data, train_review.target\n",
        "\n",
        "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
        "x_test, y_test = test_review.data, test_review.target\n",
        "\n",
        "# Display the correspondence between labels 0, 1 and their meaning\n",
        "print(train_review.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiPOmv0kqakg",
        "outputId": "ac89896d-059f-46df-d9ef-7899533f1b75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neg', 'pos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from gensim.models import Word2Vec\n",
        "    # Downloading NLTK stopwords might be necessary on the first run depending on the environment\n",
        "    # nltk.download('stopwords')\n",
        "    ENGLISH_STOP_WORDS = stopwords.words('english')\n",
        "except ImportError as e:\n",
        "    print(f\"Warning: {e}. NLTK or Gensim might not be installed.\")\n",
        "    ENGLISH_STOP_WORDS = []\n",
        "\n",
        "\n",
        "# --- 1. Loading the IMDB dataset ---\n",
        "\n",
        "def load_imdb_data(data_path='./aclImdb/'):\n",
        "    \"\"\"Function to load the IMDB dataset\"\"\"\n",
        "    print(\"--- Loading the IMDB dataset ---\")\n",
        "    try:\n",
        "        train_review = load_files(data_path + 'train/', encoding='utf-8')\n",
        "        x_train, y_train = train_review.data, train_review.target\n",
        "\n",
        "        test_review = load_files(data_path + 'test/', encoding='utf-8')\n",
        "        x_test, y_test = test_review.data, test_review.target\n",
        "\n",
        "        print(f\"Number of training data: {len(x_train)}\")\n",
        "        print(f\"Number of testing data: {len(x_test)}\")\n",
        "        print(f\"Labels ({train_review.target_names}): 0=Negative, 1=Positive\")\n",
        "        print(f\"Example of training data x_train[0]:\\n{x_train[0][:300]}...\")\n",
        "        return x_train, y_train, x_test, y_test\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n**Error:** Dataset directory not found.\")\n",
        "        print(\"Please check if the data has been downloaded and extracted to `./aclImdb/` using `!wget` and `!tar` commands.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "\n",
        "# --- 2. [Problem 1] Scratch Implementation of BoW (1-gram and 2-gram) ---\n",
        "\n",
        "def scratch_bow(sentences, ngram_range=(1, 1)):\n",
        "    \"\"\"\n",
        "    Calculates 1-gram or n-gram BoW for a given list of sentences.\n",
        "    Implementation without using scikit-learn.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- [Problem 1] Scratch Implementation of BoW ({ngram_range[0]}-gram to {ngram_range[1]}-gram) ---\")\n",
        "\n",
        "    # 1. Preprocessing and Tokenization: Lowercasing, removing non-word characters, creating word lists\n",
        "    token_lists = []\n",
        "    for sent in sentences:\n",
        "        # Lowercase and replace punctuation with spaces\n",
        "        sent = sent.lower()\n",
        "        # Replace non-alphanumeric characters (like !?. etc.) with spaces before splitting into words\n",
        "        tokens = re.findall(r'\\b\\w+\\b', sent)\n",
        "        token_lists.append(tokens)\n",
        "\n",
        "    # 2. Building vocabulary and counting n-grams\n",
        "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
        "    bow_vectors = []\n",
        "\n",
        "    for tokens in token_lists:\n",
        "        # Count dictionary\n",
        "        count_dict = defaultdict(int)\n",
        "\n",
        "        # Process from 1-gram up to n-gram\n",
        "        for n in range(ngram_range[0], ngram_range[1] + 1):\n",
        "            if n > len(tokens):\n",
        "                continue\n",
        "\n",
        "            for i in range(len(tokens) - n + 1):\n",
        "                # Generate n-gram token\n",
        "                ngram = \" \".join(tokens[i:i+n])\n",
        "\n",
        "                # Add to vocabulary (if not already in vocabulary)\n",
        "                _ = vocabulary[ngram]\n",
        "\n",
        "                # Increase count\n",
        "                count_dict[ngram] += 1\n",
        "\n",
        "        # 3. Vectorization (holding in a Dictionary for sparse representation)\n",
        "        bow_vectors.append(count_dict)\n",
        "\n",
        "    # 4. Formatting for display in DataFrame format\n",
        "\n",
        "    # Sort vocabulary list by ID (for DataFrame column order)\n",
        "    sorted_vocab = sorted(vocabulary.items(), key=lambda item: item[1])\n",
        "    column_names = [item[0] for item in sorted_vocab]\n",
        "\n",
        "    # Create BoW matrix (vocabulary size x number of samples)\n",
        "    bow_matrix = np.zeros((len(sentences), len(vocabulary)), dtype=int)\n",
        "    for i, count_dict in enumerate(bow_vectors):\n",
        "        for token, count in count_dict.items():\n",
        "            col_index = vocabulary[token]\n",
        "            bow_matrix[i, col_index] = count\n",
        "\n",
        "    df = pd.DataFrame(bow_matrix, columns=column_names, index=[f\"Doc{i}\" for i in range(len(sentences))])\n",
        "    print(df)\n",
        "    return df\n",
        "\n",
        "# --- 3. [Problem 2] TF-IDF Calculation (scikit-learn) & [Problem 3] Learning using TF-IDF ---\n",
        "\n",
        "def tfidf_classification(x_train, y_train, x_test, y_test, max_features=5000, ngram_range=(1, 1)):\n",
        "    \"\"\"\n",
        "    Performs TF-IDF vectorization and classification using Logistic Regression.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- [Problem 2 & 3] TF-IDF Vectorization and Classification (MaxFeat={max_features}, Ngram={ngram_range}) ---\")\n",
        "\n",
        "    # Use NLTK stopwords\n",
        "    stop_words = ENGLISH_STOP_WORDS\n",
        "\n",
        "    # Instantiate TfidfVectorizer\n",
        "    # Default token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b' is used, excluding single-character tokens (e.g., a, I)\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        ngram_range=ngram_range,\n",
        "        stop_words=stop_words,\n",
        "        max_features=max_features\n",
        "    )\n",
        "\n",
        "    # Vectorize training data (fit_transform)\n",
        "    X_train_tfidf = vectorizer.fit_transform(x_train)\n",
        "    # Vectorize testing data (transform)\n",
        "    X_test_tfidf = vectorizer.transform(x_test)\n",
        "\n",
        "    print(f\"TF-IDF vectorization complete. Training data dimension: {X_train_tfidf.shape}\")\n",
        "\n",
        "    # Train Logistic Regression model (binary classification)\n",
        "    model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Prediction and evaluation\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Logistic Regression test accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\n[Verification Points]\")\n",
        "    print(f\"  - Change max_features={max_features} to verify the relationship between dimension and accuracy\")\n",
        "    print(f\"  - Change ngram_range={ngram_range} to (1, 2) or (1, 3) to verify the impact of adding word order information on accuracy\")\n",
        "\n",
        "    return X_train_tfidf, X_test_tfidf\n",
        "\n",
        "# --- 4. [Problem 4] Scratch Implementation of TF-IDF ---\n",
        "\n",
        "def scratch_tfidf(sentences):\n",
        "    \"\"\"\n",
        "    Calculates standard and scikit-learn style TF-IDF for a given list of sentences.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [Problem 4] Scratch Implementation of TF-IDF ---\")\n",
        "\n",
        "    # Calculate BoW (reuse function from Problem 1)\n",
        "    bow_df = scratch_bow(sentences, ngram_range=(1, 1))\n",
        "    bow_matrix = bow_df.values\n",
        "    vocab = bow_df.columns.tolist()\n",
        "    N = len(sentences) # Number of samples\n",
        "\n",
        "    # 1. TF Calculation (scikit-learn style: n_t,d)\n",
        "    # Scikit-learn's TF is simply the term frequency (same as BoW)\n",
        "    tf_sklearn = bow_matrix\n",
        "\n",
        "    # 2. Standard TF Calculation (n_t,d / sum(n_s,d))\n",
        "    # Sum of all token occurrences in each sample (denominator)\n",
        "    sum_tokens_per_doc = bow_matrix.sum(axis=1, keepdims=True)\n",
        "    # Avoid division by zero when denominator is 0\n",
        "    sum_tokens_per_doc[sum_tokens_per_doc == 0] = 1\n",
        "    tf_standard = bow_matrix / sum_tokens_per_doc\n",
        "\n",
        "    # 3. Required for IDF calculation: df(t) (number of samples where token t appears)\n",
        "    df_t = np.sum(bow_matrix > 0, axis=0)\n",
        "\n",
        "    # 4. Standard IDF Calculation (log(N / df(t)))\n",
        "    # Base of the logarithm is arbitrary, natural logarithm (e) is used here\n",
        "    idf_standard = np.log(N / df_t)\n",
        "\n",
        "    # 5. Scikit-learn's IDF Calculation (log((1 + N) / (1 + df(t))) + 1)\n",
        "    idf_sklearn = np.log((1 + N) / (1 + df_t)) + 1\n",
        "\n",
        "    # 6. TF-IDF Calculation\n",
        "    tfidf_standard = tf_standard * idf_standard\n",
        "    tfidf_sklearn = tf_sklearn * idf_sklearn\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n[Standard TF-IDF Results]\")\n",
        "    df_standard = pd.DataFrame(tfidf_standard, columns=vocab, index=[f\"Doc{i}\" for i in range(N)])\n",
        "    print(df_standard.round(4))\n",
        "\n",
        "    print(\"\\n[Scikit-learn style TF-IDF Results (without normalization)]\")\n",
        "    df_sklearn = pd.DataFrame(tfidf_sklearn, columns=vocab, index=[f\"Doc{i}\" for i in range(N)])\n",
        "    print(df_sklearn.round(4))\n",
        "\n",
        "\n",
        "# --- 5. [Problem 5] Corpus Preprocessing (for Word2Vec) ---\n",
        "\n",
        "def preprocess_for_word2vec(text_list):\n",
        "    \"\"\"\n",
        "    Preprocessing for Word2Vec training: URL removal, special character removal, lowercasing, token splitting.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [Problem 5] Corpus Preprocessing for Word2Vec ---\")\n",
        "    processed_sentences = []\n",
        "\n",
        "    for text in text_list:\n",
        "        # 1. Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # 2. Remove HTML tags (may be present in IMDB data)\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "        # 3. Remove special characters and lowercase (replace non-alphabetic and non-numeric characters with spaces)\n",
        "        # Punctuation is removed as word distance is important in Word2Vec\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text.lower())\n",
        "\n",
        "        # 4. Split into tokens (consecutive spaces are treated as one)\n",
        "        tokens = [word for word in text.split() if word]\n",
        "        processed_sentences.append(tokens)\n",
        "\n",
        "    print(f\"Example of processed sentence (first 5 words): {processed_sentences[0][:5]}...\")\n",
        "    return processed_sentences\n",
        "\n",
        "# --- 6. [Problem 6] Word2Vec Training & [Problem 7] Similar Words and Visualization ---\n",
        "\n",
        "def train_and_visualize_word2vec(sentences):\n",
        "    \"\"\"\n",
        "    Trains Word2Vec, searches for similar words, and visualizes (TSNE).\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [Problem 6] Word2Vec Training ---\")\n",
        "    # min_count=5: Ignore words with frequency less than 5\n",
        "    # vector_size=100: Set the dimension of distributed representation to 100\n",
        "    # window=5: Maximum distance between the current and predicted word within a sentence\n",
        "    # workers=4: Use 4 parallel workers\n",
        "    # sg=1: Use Skip-gram (default is sg=0 for CBoW)\n",
        "    model = Word2Vec(sentences, min_count=5, vector_size=100, window=5, workers=4, sg=1)\n",
        "\n",
        "    print(f\"Number of words in vocabulary: {len(model.wv.index_to_key)}\")\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # [Problem 7] Vector Visualization and Similar Words\n",
        "    print(\"\\n--- [Problem 7] Preparing for Similar Word Search and Visualization ---\")\n",
        "\n",
        "    # Search for similar words\n",
        "    try:\n",
        "        similar_words = model.wv.most_similar(positive=['good'], topn=5)\n",
        "        print(\"Top 5 words most similar to 'good':\")\n",
        "        for word, sim in similar_words:\n",
        "            print(f\"  - {word}: {sim:.4f}\")\n",
        "\n",
        "        similar_words_bad = model.wv.most_similar(positive=['bad'], topn=5)\n",
        "        print(\"\\nTop 5 words most similar to 'bad':\")\n",
        "        for word, sim in similar_words_bad:\n",
        "            print(f\"  - {word}: {sim:.4f}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Error during similar word search: {e}. This word is not in the vocabulary.\")\n",
        "\n",
        "    # Prepare for visualization (t-SNE)\n",
        "    words = model.wv.index_to_key[:100] # Select top 100 words\n",
        "    vectors = model.wv[words]\n",
        "\n",
        "    print(f\"\\nRunning TSNE: Compressing vectors of top {len(words)} words to 2 dimensions...\")\n",
        "    tsne_model = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, random_state=42)\n",
        "    vectors_tsne = tsne_model.fit_transform(vectors)\n",
        "\n",
        "    # Plot (assuming execution in Colab/Jupyter environment)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, word in enumerate(words):\n",
        "        plt.scatter(vectors_tsne[i, 0], vectors_tsne[i, 1])\n",
        "        plt.annotate(word,\n",
        "                     xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.title(\"Word2Vec Vector t-SNE Visualization (Top 100 Words)\")\n",
        "    plt.xlabel(\"TSNE Dimension 1\")\n",
        "    plt.ylabel(\"TSNE Dimension 2\")\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- 7. [Problem 8] Movie Review Classification using Word2Vec (Advanced) ---\n",
        "\n",
        "def w2v_classification(x_train_processed, y_train, x_test_processed, y_test, w2v_model):\n",
        "    \"\"\"\n",
        "    Creates sentence vectors by averaging Word2Vec word vectors and performs classification.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [Problem 8] Sentiment Classification using Word2Vec ---\")\n",
        "\n",
        "    def document_to_vector(doc_tokens, model, vector_size):\n",
        "        \"\"\"Calculates the average of all word vectors in a review to create a sentence vector\"\"\"\n",
        "        vectors = []\n",
        "        for word in doc_tokens:\n",
        "            if word in model.wv:\n",
        "                vectors.append(model.wv[word])\n",
        "\n",
        "        if vectors:\n",
        "            # Average of existing word vectors\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            # Return a zero vector if the document consists only of words not in the vocabulary\n",
        "            return np.zeros(vector_size)\n",
        "\n",
        "    VECTOR_SIZE = w2v_model.wv.vector_size\n",
        "\n",
        "    # Convert training and testing data to sentence vectors\n",
        "    X_train_w2v = np.array([document_to_vector(tokens, w2v_model, VECTOR_SIZE) for tokens in x_train_processed])\n",
        "    X_test_w2v = np.array([document_to_vector(tokens, w2v_model, VECTOR_SIZE) for tokens in x_test_processed])\n",
        "\n",
        "    print(f\"Word2Vec sentence vectorization complete. Training data dimension: {X_train_w2v.shape}\")\n",
        "\n",
        "    # Train Logistic Regression model (binary classification)\n",
        "    model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n",
        "    model.fit(X_train_w2v, y_train)\n",
        "\n",
        "    # Prediction and evaluation\n",
        "    y_pred = model.predict(X_test_w2v)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Word2Vec + Logistic Regression test accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 0. Load the IMDB dataset\n",
        "    x_train, y_train, x_test, y_test = load_imdb_data()\n",
        "\n",
        "    if x_train is None:\n",
        "        # Exit on data loading error\n",
        "        exit()\n",
        "\n",
        "    # --- 1. Scratch Implementation of Classical Methods ---\n",
        "    mini_dataset = [\n",
        "      \"This movie is SOOOO funny!!!\",\n",
        "      \"What a movie! I never\",\n",
        "      \"best movie ever!!!!! this movie\"\n",
        "    ]\n",
        "\n",
        "    # [Problem 1] Scratch Implementation of BoW\n",
        "    scratch_bow(mini_dataset, ngram_range=(1, 1))\n",
        "    scratch_bow(mini_dataset, ngram_range=(2, 2))\n",
        "\n",
        "\n",
        "    # --- 2. TF-IDF Calculation and Learning (scikit-learn) ---\n",
        "\n",
        "    # [Problem 2 & 3] TF-IDF Calculation and Classification\n",
        "    # Execute with max vocabulary size 5000 and 1-gram only\n",
        "    X_train_tfidf, X_test_tfidf = tfidf_classification(x_train, y_train, x_test, y_test, max_features=5000, ngram_range=(1, 1))\n",
        "\n",
        "\n",
        "    # --- 3. Scratch Implementation of TF-IDF ---\n",
        "\n",
        "    # [Problem 4] Scratch Implementation of TF-IDF\n",
        "    scratch_tfidf(mini_dataset)\n",
        "\n",
        "\n",
        "    # --- 4. Distributed Representation using Word2Vec ---\n",
        "\n",
        "    # [Problem 5] Corpus Preprocessing\n",
        "    x_train_processed = preprocess_for_word2vec(x_train)\n",
        "    x_test_processed = preprocess_for_word2vec(x_test) # Preprocess test data similarly\n",
        "\n",
        "    # [Problem 6, 7] Word2Vec Training, Similar Words, Visualization\n",
        "    w2v_model = train_and_visualize_word2vec(x_train_processed)\n",
        "\n",
        "    # [Problem 8] Classification using Word2Vec\n",
        "    w2v_classification(x_train_processed, y_train, x_test_processed, y_test, w2v_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "d_xJJo7zqp9R",
        "outputId": "7e152a31-575d-4631-a302-93b9d166dcda"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3939191497.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Downloading NLTK stopwords might be necessary on the first run depending on the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# nltk.download('stopwords')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ffClTypyrjAB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}