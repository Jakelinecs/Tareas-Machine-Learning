{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3raxAztvugInZOlYAs3XZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jakelinecs/Tareas-Machine-Learning/blob/main/N23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#   P1\n",
        "class FC:\n",
        "\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = self.initializer.B(self.n_nodes2)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        self.X = X\n",
        "        return np.dot(X, self.W) + self.B\n",
        "\n",
        "    def backward(self, dA):\n",
        "\n",
        "        batch_size = self.X.shape[0]\n",
        "        self.dW = np.dot(self.X.T, dA)\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "        return dZ\n",
        "\n",
        "\n",
        "#   P2\n",
        "\n",
        "class SimpleInitializer:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "\n",
        "#  P6\n",
        "class XavierInitializer:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        sigma = np.sqrt(1 / n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "\n",
        "\n",
        "class HeInitializer:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        sigma = np.sqrt(2 / n_nodes1)\n",
        "        return sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        return np.zeros(n_nodes2)\n",
        "\n",
        "#   P3\n",
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return layer\n",
        "\n",
        "#   p7\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.H = {}\n",
        "\n",
        "    def update(self, layer):\n",
        "\n",
        "        if id(layer) not in self.H:\n",
        "            self.H[id(layer)] = {\n",
        "                'HW' : np.zeros_like(layer.W),\n",
        "                'HB' : np.zeros_like(layer.B)\n",
        "            }\n",
        "\n",
        "        HW = self.H[id(layer)]['HW']\n",
        "        HB = self.H[id(layer)]['HB']\n",
        "\n",
        "        HW += layer.dW ** 2\n",
        "        HB += layer.dB ** 2\n",
        "\n",
        "\n",
        "        layer.W -= self.lr * (1 / (np.sqrt(HW) + 1e-7)) * layer.dW\n",
        "        layer.B -= self.lr * (1 / (np.sqrt(HB) + 1e-7)) * layer.dB\n",
        "        return layer\n",
        "\n",
        "\n",
        "#   P4 and P5\n",
        "    \"\"\"Función Activacion\"\"\"\n",
        "class Tanh:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        # Derivada de tanh(A): 1 - tanh^2(A)\n",
        "        return dZ * (1 - np.tanh(self.A)**2)\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return 1/ (1 + np.exp(-A))\n",
        "\n",
        "    def backward(self, dZ):\n",
        "\n",
        "        sig = 1/ (1 + np.exp(-self.A))\n",
        "        return dZ * sig * (1 - sig)\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    \"\"\"ReLU (Rectified Linear Unit)\"\"\"\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.maximum(0, A)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (self.A > 0 )\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    \"\"\"Función Softmax (Capa de Salida)\"\"\"\n",
        "    def forward(self, A):\n",
        "\n",
        "        exp_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
        "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
        "        return self.Z\n",
        "\n",
        "\n",
        "    def backward(self, Z, Y):\n",
        "\n",
        "        batch_size = Y.shape[0]\n",
        "        dL_dA = (Z - Y) / batch_size\n",
        "        return dL_dA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   P8\n",
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "\n",
        "    def __init__(self, layers_config, initializer='simple', optimizer = 'sgd',\n",
        "                 sigma=0.01, lr=0.01, epochs = 10,\n",
        "                 batch_size=20, verbose=True):\n",
        "\n",
        "\n",
        "        self.layers_config = layers_config\n",
        "        self.sigma = sigma\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self._initialize_layers(initializer, optimizer)\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "\n",
        "\n",
        "    def _initialize_layers(self, initializer, optimizer):\n",
        "\n",
        "\n",
        "        if initializer == 'simple':\n",
        "            initializer = SimpleInitializer(self.sigma)\n",
        "        elif initializer == 'xavier':\n",
        "            initializer = XavierInitializer()\n",
        "        elif initializer == 'he':\n",
        "            initializer = HeInitializer()\n",
        "\n",
        "        if optimizer == 'sgd':\n",
        "            optimizer = SGD(self.lr)\n",
        "        elif optimizer == 'adagrad':\n",
        "            optimizer = AdaGrad(self.lr)\n",
        "\n",
        "        self.layers = []\n",
        "        self.activations = []\n",
        "\n",
        "        for i, config in enumerate(self.layers_config):\n",
        "\n",
        "            n_nodes1 = config['input_dim'] if i == 0 else self.layers_config[i-1]['output_dim']\n",
        "            n_nodes2 = config['output_dim']\n",
        "            self.layers.append(FC(n_nodes1, n_nodes2, initializer, optimizer))\n",
        "\n",
        "\n",
        "            if config['activation'] == 'tanh':\n",
        "                self.activations.append(Tanh())\n",
        "            elif config['activation'] == 'sigmoid':\n",
        "                self.activations.append(Sigmoid())\n",
        "            elif config['activation'] == 'relu':\n",
        "                self.activations.append(ReLU())\n",
        "            elif config['activation'] == 'softmax':\n",
        "                self.activations.append(Softmax())\n",
        "\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "\n",
        "      for epoch in range(self.epochs):\n",
        "        get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "\n",
        "        for mini_X, mini_y in get_mini_batch:\n",
        "\n",
        "          A = mini_X\n",
        "          for layer, activation in zip(self.layers, self.activations):\n",
        "            A = layer.forward(A)\n",
        "            Z = activation.forward(A)\n",
        "            A = Z\n",
        "\n",
        "          dA = self.activations[-1].backward(Z, mini_y)\n",
        "          for i in reversed(range(len(self.layers))):\n",
        "            dZ = self.layers[i].backward(dA)\n",
        "            if i > 0:\n",
        "              dA = self.activations[i-1].backward(dA)\n",
        "\n",
        "        train_loss = self.calc_loss(X, y)\n",
        "        self.train_loss.append(train_loss)\n",
        "\n",
        "        if X_val is not None and y_val is not None:\n",
        "          val_loss = self.calc_loss(X_val, y_val)\n",
        "          self.val_loss.append(val_loss)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs}, Train Loss: {train_loss:.4f}\",\n",
        "                  f\", Val Loss: {val_loss:.4f}\" if X_val is not None and y_val is not None else \"\")\n",
        "\n",
        "    def _calc_loss(self, X, y):\n",
        "        A = X\n",
        "        for layer, activation in zip(self.layers, self.activations):\n",
        "            A = layer.forward(A)\n",
        "            Z = self.activation.forward(A)\n",
        "            A = Z\n",
        "\n",
        "        delta = 1e-7\n",
        "        return -np.sum(y * np.log(Z + delta)) / y.shape[0]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "      A = X\n",
        "      for layer, activation in zip(self.layers, self.activations):\n",
        "        A = layer.forward(A)\n",
        "        Z = activation.forward(A)\n",
        "        A = Z\n",
        "\n",
        "      return np.argmax(Z, axis=1)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "      A = X\n",
        "      for layer, activation in zip(self.layers, self.activations):\n",
        "        A = layer.forward(A)\n",
        "        Z = activation.forward(A)\n",
        "        A = Z\n",
        "\n",
        "      return Z\n",
        "\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = int(np.ceil(X.shape[0]/self.batch_size))\n",
        "        self._counter = 0\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "\n",
        "#     P9\n",
        "\n",
        "    def load_and_preprocess_data():\n",
        "      (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "      X_train = X_train.reshape(-1, 784).astype(np.float32) / 255\n",
        "      X_test = X_test.reshape(-1, 784).astype(np.float32) / 255\n",
        "\n",
        "      try:\n",
        "        enc = OneHotEncoder( sparse=False)\n",
        "      except TypeError:\n",
        "        enc = OneHotEncoder( sparse_output = False )\n",
        "      y_train_one_hot = enc.fit_transform(y_train.reshape(-1, 1))\n",
        "      y_test_one_hot = enc.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "\n",
        "      X_train, X_val, y_train, y_val = train_test_split(\n",
        "          X_train, y_train_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "      return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_model(model, X_val, y_val):\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_val_labels = np.argmax(y_val, axis=1)\n",
        "        accuracy = np.mean(y_pred == y_val_labels)\n",
        "\n",
        "        print(f\"Validation Accuracy: {accuracy: .4f}\")\n",
        "\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(model.train_loss, label='Train Loss')\n",
        "        if model.val_loss:\n",
        "            plt.plot(model.val_loss, label='Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Learning Curve')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def visualize_misclassifications(model, X_val, y_val, num=36):\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_val_labels = np.argmax(y_val, axis=1)\n",
        "\n",
        "\n",
        "        true_false = y_pred == y_val_labels\n",
        "        false_list = np.where(true_false == False)[0].astype(int)\n",
        "\n",
        "\n",
        "        if false_list.shape[0] < num:\n",
        "            num = false_list.shape[0]\n",
        "\n",
        "        fig = plt.figure(figsize=(6, 6))\n",
        "        fig.subplots_adjust(left=0, right=0.8, bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
        "        for i in range(num):\n",
        "            ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
        "            ax.set_title(\"{} / {}\".format(y_pred[false_list[i]], y_val_labels[false_list[i]]))\n",
        "            ax.imshow(X_val.reshape(-1, 28, 28)[false_list[i]], cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = load_and_preprocess_data()\n",
        "\n",
        "        # conf P 9\n",
        "    configuracion1 = [\n",
        "        {'input_dim': 784, 'output_dim': 400, 'activation': 'tanh'},\n",
        "        {'input_dim': 400, 'output_dim': 200, 'activation': 'tanh'},\n",
        "        {'input_dim': 200, 'output_dim': 10, 'activation': 'softmax'}\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    configuracion2 = [\n",
        "        {'input_dim': 784, 'output_dim': 400, 'activation': 'relu'},\n",
        "        {'input_dim': 400, 'output_dim': 200, 'activation': 'relu'},\n",
        "        {'input_dim': 200, 'output_dim': 10, 'activation': 'softmax'}\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    configuracion3 = [\n",
        "        {'input_dim': 784, 'output_dim': 512, 'activation': 'relu'},\n",
        "        {'input_dim': 512, 'output_dim': 256, 'activation': 'relu'},\n",
        "        {'input_dim': 256, 'output_dim': 128, 'activation': 'relu'},\n",
        "        {'input_dim': 128, 'output_dim': 10, 'activation': 'softmax'}\n",
        "    ]\n",
        "\n",
        "\n",
        "    print(\"Model 1: Tank activation with simple initialition and SGD\")\n",
        "\n",
        "    model1 = ScratchDeepNeuralNetrowkClassifier(\n",
        "        layers_config=configuracion1,\n",
        "        initializer='simple',\n",
        "        optimizer='sgd',\n",
        "        sigma=0.01,\n",
        "        lr=0.01,\n",
        "        epochs=10,\n",
        "        batch_size=20\n",
        "    )\n",
        "\n",
        "\n",
        "    model1.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    evaluate_model(model1, X_val, y_val)\n",
        "    visualize_misclassifications(model1, X_val, y_val)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nModel 2: Relu activation \")\n",
        "    model2 = ScratchDeepNeuralNetrowkClassifier(\n",
        "        layers_config=configuracion2,\n",
        "        initializer='he',\n",
        "        optimizer='adagrad',\n",
        "        lr=0.01,\n",
        "        epochs=10,\n",
        "        batch_size=20\n",
        "    )\n",
        "\n",
        "\n",
        "    model2.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    evaluate_model(model2, X_val, y_val)\n",
        "    visualize_misclassifications(model2, X_val, y_val)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nModel 3: Deeper activation \")\n",
        "    model3 = ScratchDeepNeuralNetrowkClassifier(\n",
        "        layers_config=configuracion3,\n",
        "        initializer='he',\n",
        "        optimizer='adagrad',\n",
        "        lr=0.01,\n",
        "        epochs=10,\n",
        "        batch_size=10\n",
        "    )\n",
        "\n",
        "\n",
        "    model3.fit(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    evaluate_model(model3, X_val, y_val)\n",
        "    visualize_misclassifications(model3, X_val, y_val)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "o_LaOh9qkxB0",
        "outputId": "31357c74-4d28-4e7e-9093-2ccefdc7ef96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1: Tank activation with simple initialition and SGD\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GetMiniBatch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-544468800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGetMiniBatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-544468800.py\u001b[0m in \u001b[0;36mGetMiniBatch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-544468800.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mget_mini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetMiniBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmini_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_mini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GetMiniBatch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eEDMzf45mGH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5WoYqD1mlHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}