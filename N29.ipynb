{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvsRDp91DrbmnHEzOYjR6G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jakelinecs/Tareas-Machine-Learning/blob/main/N29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caea770a",
        "outputId": "40dc4d44-d9f6-456a-bdc5-5e7099b7c50c"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Specify the path to your zip file\n",
        "zip_file_path = 'train.zip' # Replace with the actual path to your zip file\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extraction_dir = '/content/train' # You can change this to your desired directory\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_dir, exist_ok=True)\n",
        "\n",
        "# Open the zip file in read mode\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the specified directory\n",
        "    zip_ref.extractall(extraction_dir)\n",
        "\n",
        "print(f\"Successfully extracted '{zip_file_path}' to '{extraction_dir}'\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted 'train.zip' to '/content/train'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LyTgO77fLWF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3f9c5b1-4d81-49e4-fd12-afea5c58efa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Prueba de Carga de Datos ---\n",
            "Buscando datos en: ./train/\n",
            "✓ Directorio de datos encontrado\n",
            "✓ Directorio 'images' encontrado\n",
            "  - 4000 archivos de imagen encontrados\n",
            "✓ Directorio 'masks' encontrado\n",
            "  - 4000 archivos de máscara encontrados\n",
            "\n",
            "--- Intentando cargar datos ---\n",
            "Buscando datos en: ./train/\n",
            "Total imágenes encontradas: 4000\n",
            "Train samples: 2400\n",
            "Validation samples: 800\n",
            "Test samples: 800\n",
            "✓ Generadores creados exitosamente\n",
            "\n",
            "Shape de X (Imágenes): (16, 128, 128, 1)\n",
            "Shape de y (Máscaras): (16, 128, 128, 1)\n",
            "Valores Máx/Mín de X: 0.00 / 1.00\n",
            "Valores Máx/Mín de y: 0.00 / 257.00\n",
            "Número de batches por época: 150\n",
            "\n",
            "--- Información del Script ---\n",
            "Este script está diseñado para el TGS Salt Identification Challenge de Kaggle\n",
            "Funcionalidades:\n",
            "  - Carga imágenes y máscaras de segmentación\n",
            "  - Redimensiona de 101x101 a 128x128 píxeles\n",
            "  - Normaliza valores a [0,1]\n",
            "  - Divide datos en train/validation/test\n",
            "  - Genera batches para entrenamiento con Keras\n"
          ]
        }
      ],
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- CONFIGURACIÓN DE RUTAS Y PARÁMETROS ---\n",
        "\n",
        "# Directorio base donde se encuentran las carpetas 'images' y 'masks' del TGS Salt Challenge\n",
        "# ¡ATENCIÓN!: Reemplace esta ruta con la ubicación real de sus datos de Kaggle\n",
        "DATA_DIR = './train/'\n",
        "\n",
        "# Tamaño de imagen de entrada original (TGS)\n",
        "IMG_SIZE_ORIG = 101\n",
        "# Tamaño objetivo para U-Net (idealmente potencia de 2, como 128)\n",
        "IMG_SIZE_TARGET = 128\n",
        "# Número de canales de entrada (Imágenes en escala de grises = 1)\n",
        "IMG_CHANNELS = 1\n",
        "\n",
        "# --- GENERADOR DE DATOS DE KERAS ---\n",
        "\n",
        "class TGSDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Generador de datos personalizado de Keras para cargar imágenes y máscaras en batches.\n",
        "    Aplica el redimensionamiento (padding de 101x101 a 128x128) y normalización.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, batch_size=16, shuffle=True, augment=False):\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment # Futura implementación de aumento de datos\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Número de batches por época\"\"\"\n",
        "        return int(np.floor(len(self.df) / self.batch_size))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Mezcla los índices después de cada época si shuffle está activado\"\"\"\n",
        "        self.indexes = np.arange(len(self.df))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Genera un batch de datos (X e Y)\"\"\"\n",
        "        # Obtiene los IDs de las imágenes para el batch actual\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_IDs_temp = self.df.iloc[indexes]['id'].tolist()\n",
        "\n",
        "        # Genera los datos\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        return X, y\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        \"\"\"Genera el contenido real del batch\"\"\"\n",
        "        X = np.empty((self.batch_size, IMG_SIZE_TARGET, IMG_SIZE_TARGET, IMG_CHANNELS), dtype=np.float32)\n",
        "        y = np.empty((self.batch_size, IMG_SIZE_TARGET, IMG_SIZE_TARGET, IMG_CHANNELS), dtype=np.float32)\n",
        "\n",
        "        for i, id_name in enumerate(list_IDs_temp):\n",
        "            # 1. Carga de la imagen (X)\n",
        "            img = imread(os.path.join(DATA_DIR, 'images', id_name + '.png'))\n",
        "            # Escala de grises (101, 101, 1). Se extrae el primer canal si se carga como RGB\n",
        "            img = img[:, :, 0] if img.ndim == 3 else img\n",
        "\n",
        "            # 2. Carga de la máscara (Y)\n",
        "            mask = imread(os.path.join(DATA_DIR, 'masks', id_name + '.png'))\n",
        "            mask = mask[:, :, 0] if mask.ndim == 3 else mask\n",
        "\n",
        "            # 3. Redimensionamiento y Padding (101x101 -> 128x128)\n",
        "            # Nota: resize aplica internamente el padding necesario si el tamaño de salida es mayor.\n",
        "            # Usaremos resize para escalar y luego expandir la dimensión de canal.\n",
        "\n",
        "            # Redimensionar la imagen y la máscara al tamaño objetivo (128x128)\n",
        "            img_resized = resize(img, (IMG_SIZE_TARGET, IMG_SIZE_TARGET), mode='constant', preserve_range=True)\n",
        "            mask_resized = resize(mask, (IMG_SIZE_TARGET, IMG_SIZE_TARGET), mode='constant', preserve_range=True)\n",
        "\n",
        "            # 4. Normalización\n",
        "            # Las imágenes se normalizan a [0, 1]\n",
        "            X[i,] = (img_resized / 255.0).reshape(IMG_SIZE_TARGET, IMG_SIZE_TARGET, IMG_CHANNELS)\n",
        "            # La máscara (etiquetas binarias) se normaliza a [0, 1]\n",
        "            y[i,] = (mask_resized / 255.0).reshape(IMG_SIZE_TARGET, IMG_SIZE_TARGET, IMG_CHANNELS)\n",
        "\n",
        "\n",
        "        return X, y\n",
        "\n",
        "# --- FUNCIÓN PRINCIPAL DE CARGA DE DATOS ---\n",
        "\n",
        "def load_tgs_data(data_dir=DATA_DIR, batch_size=16, test_size=0.2, val_size=0.2):\n",
        "    \"\"\"\n",
        "    Carga los IDs, divide el conjunto de datos y crea los generadores.\n",
        "\n",
        "    Returns:\n",
        "        train_generator, val_generator, test_generator\n",
        "    \"\"\"\n",
        "    print(f\"Buscando datos en: {data_dir}\")\n",
        "    # Cargar IDs de imágenes\n",
        "    all_files = [f.split('.')[0] for f in os.listdir(os.path.join(data_dir, 'images')) if f.endswith('.png')]\n",
        "    df_ids = pd.DataFrame({'id': all_files})\n",
        "\n",
        "    if len(df_ids) == 0:\n",
        "        print(\"ERROR: No se encontraron archivos de imagen. Verifique la ruta DATA_DIR.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # División: Train y Temp (para Val y Test)\n",
        "    train_df, temp_df = train_test_split(df_ids, test_size=(test_size + val_size), random_state=42)\n",
        "\n",
        "    # División: Val y Test\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=test_size/(test_size + val_size), random_state=42)\n",
        "\n",
        "    print(f\"Total imágenes encontradas: {len(df_ids)}\")\n",
        "    print(f\"Train samples: {len(train_df)}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "    print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "    # Creación de generadores\n",
        "    train_gen = TGSDataGenerator(train_df, batch_size=batch_size, shuffle=True)\n",
        "    val_gen = TGSDataGenerator(val_df, batch_size=batch_size, shuffle=False)\n",
        "    test_gen = TGSDataGenerator(test_df, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_gen, val_gen, test_gen\n",
        "\n",
        "# --- EJEMPLO DE USO (SOLO PARA PRUEBA) ---\n",
        "if __name__ == '__main__':\n",
        "    # Nota: Este bloque de código solo funcionará si los datos están presentes en DATA_DIR\n",
        "    print(\"--- Prueba de Carga de Datos ---\")\n",
        "    print(f\"Buscando datos en: {DATA_DIR}\")\n",
        "\n",
        "    # Verificar si el directorio existe\n",
        "    if os.path.exists(DATA_DIR):\n",
        "        print(\"✓ Directorio de datos encontrado\")\n",
        "\n",
        "        # Verificar subdirectorios\n",
        "        images_dir = os.path.join(DATA_DIR, 'images')\n",
        "        masks_dir = os.path.join(DATA_DIR, 'masks')\n",
        "\n",
        "        if os.path.exists(images_dir):\n",
        "            print(\"✓ Directorio 'images' encontrado\")\n",
        "            image_files = [f for f in os.listdir(images_dir) if f.endswith('.png')]\n",
        "            print(f\"  - {len(image_files)} archivos de imagen encontrados\")\n",
        "        else:\n",
        "            print(\"✗ Directorio 'images' no encontrado\")\n",
        "\n",
        "        if os.path.exists(masks_dir):\n",
        "            print(\"✓ Directorio 'masks' encontrado\")\n",
        "            mask_files = [f for f in os.listdir(masks_dir) if f.endswith('.png')]\n",
        "            print(f\"  - {len(mask_files)} archivos de máscara encontrados\")\n",
        "        else:\n",
        "            print(\"✗ Directorio 'masks' no encontrado\")\n",
        "\n",
        "        # Si ambos directorios existen, intentar cargar los datos\n",
        "        if os.path.exists(images_dir) and os.path.exists(masks_dir):\n",
        "            print(\"\\n--- Intentando cargar datos ---\")\n",
        "            try:\n",
        "                train_gen, val_gen, test_gen = load_tgs_data(batch_size=16)\n",
        "\n",
        "                if train_gen:\n",
        "                    print(\"✓ Generadores creados exitosamente\")\n",
        "                    X_batch, y_batch = train_gen.__getitem__(0)\n",
        "                    print(f\"\\nShape de X (Imágenes): {X_batch.shape}\")\n",
        "                    print(f\"Shape de y (Máscaras): {y_batch.shape}\")\n",
        "                    print(f\"Valores Máx/Mín de X: {X_batch.min():.2f} / {X_batch.max():.2f}\")\n",
        "                    print(f\"Valores Máx/Mín de y: {y_batch.min():.2f} / {y_batch.max():.2f}\")\n",
        "                    print(f\"Número de batches por época: {len(train_gen)}\")\n",
        "                else:\n",
        "                    print(\"✗ No se pudieron crear los generadores\")\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error al cargar datos: {e}\")\n",
        "        else:\n",
        "            print(\"\\n⚠️  No se pueden cargar los datos porque faltan directorios\")\n",
        "    else:\n",
        "        print(\"✗ Directorio de datos no encontrado\")\n",
        "        print(f\"  Ruta configurada: {DATA_DIR}\")\n",
        "        print(\"  Para usar este script, descargue el dataset TGS Salt Challenge de Kaggle\")\n",
        "        print(\"  y ajuste la variable DATA_DIR en el script\")\n",
        "\n",
        "    print(\"\\n--- Información del Script ---\")\n",
        "    print(\"Este script está diseñado para el TGS Salt Identification Challenge de Kaggle\")\n",
        "    print(\"Funcionalidades:\")\n",
        "    print(\"  - Carga imágenes y máscaras de segmentación\")\n",
        "    print(\"  - Redimensiona de 101x101 a 128x128 píxeles\")\n",
        "    print(\"  - Normaliza valores a [0,1]\")\n",
        "    print(\"  - Divide datos en train/validation/test\")\n",
        "    print(\"  - Genera batches para entrenamiento con Keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSuIQ3U3BbTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}