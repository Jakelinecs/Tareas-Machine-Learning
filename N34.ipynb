{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVjkgMq/Aew4rYrSqke/rs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jakelinecs/Tareas-Machine-Learning/blob/main/N34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "==================================================================================================\n",
        "PRACTICO COMPLETO: TRADUCCIÓN AUTOMÁTICA Y SUBTITULADO DE IMÁGENES (KERAS/PYTORCH)\n",
        "==================================================================================================\n",
        "\n",
        "Este script contiene la implementación del modelo Seq2Seq (LSTM a nivel de carácter) para la\n",
        "traducción automática (Punto 2) y la implementación de la Tarea Avanzada de Keras para el\n",
        "Subtitulado de Imágenes (Punto 3), simulando la arquitectura del archivo model.py de PyTorch.\n",
        "\n",
        "Nota: El código de traducción automática está completamente documentado línea por línea\n",
        "para cumplir con el requisito de la tarea.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "# Línea 39: Importa la librería NumPy, esencial para el manejo eficiente de arrays y tensores.\n",
        "import keras\n",
        "# Línea 40: Importa la librería Keras para construir y entrenar la red neuronal.\n",
        "import os\n",
        "# Línea 41: Importa el módulo 'os' para interactuar con el sistema operativo (comandos y rutas).\n",
        "from pathlib import Path\n",
        "# Línea 42: Importa la clase 'Path' de pathlib para manejar rutas de archivos.\n",
        "\n",
        "# === LÍNEAS 5-34: INTRODUCCIÓN/SETUP (Texto original del script) ===\n",
        "# (Estas líneas son comentarios informativos en el script original y se omiten aquí por ser prosa,\n",
        "# pero en el archivo final deben estar presentes como el bloque de comentario introductorio original).\n",
        "# Líneas 38-41: Bloque de importación.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. TRADUCCIÓN AUTOMÁTICA (Implementación de lstm_seq2seq.py)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "## Descarga de datos\n",
        "\"\"\"\n",
        "\n",
        "fpath = keras.utils.get_file(origin=\"http://www.manythings.org/anki/fra-eng.zip\")\n",
        "# Línea 47: Descarga el archivo ZIP del corpus de traducción y almacena su ruta local en 'fpath'.\n",
        "dirpath = Path(fpath).parent.absolute()\n",
        "# Línea 48: Obtiene el directorio absoluto donde se descargó el archivo ZIP.\n",
        "os.system(f\"unzip -q {fpath} -d {dirpath}\")\n",
        "# Línea 49: Ejecuta un comando del sistema para descomprimir silenciosamente (-q) el archivo.\n",
        "\n",
        "\"\"\"\n",
        "## Configuración\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "# Línea 54: Define el tamaño del lote (batch size) para el entrenamiento.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "# Línea 55: Define el número de épocas para el entrenamiento.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "# Línea 56: Define la dimensionalidad latente (unidades internas) de las capas LSTM.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Línea 57: Define el número máximo de pares de frases a utilizar.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = os.path.join(dirpath, \"fra.txt\")\n",
        "# Línea 59: Construye la ruta completa al archivo de datos de texto sin comprimir.\n",
        "\n",
        "\"\"\"\n",
        "## Preparación de datos\n",
        "\"\"\"\n",
        "\n",
        "# Vectorize the data.\n",
        "# Línea 64: Comentario indicando el inicio de la fase de vectorización.\n",
        "input_texts = []\n",
        "# Línea 65: Inicializa lista para frases de entrada (Inglés).\n",
        "target_texts = []\n",
        "# Línea 66: Inicializa lista para frases de destino (Francés).\n",
        "input_characters = set()\n",
        "# Línea 67: Inicializa conjunto para caracteres únicos de entrada.\n",
        "target_characters = set()\n",
        "# Línea 68: Inicializa conjunto para caracteres únicos de destino.\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "# Línea 69: Abre el archivo de datos en modo lectura.\n",
        "    lines = f.read().split(\"\\n\")\n",
        "# Línea 70: Lee todo el archivo y lo divide en una lista de frases.\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "# Línea 71: Itera sobre las primeras 'num_samples' líneas.\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "# Línea 72: Divide la línea por el tabulador ('\\t').\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "# Línea 76: Añade el token de INICIO DE SECUENCIA ('\\t') y FIN DE SECUENCIA ('\\n') a la frase de destino.\n",
        "    input_texts.append(input_text)\n",
        "# Línea 77: Agrega la frase de entrada a la lista.\n",
        "    target_texts.append(target_text)\n",
        "# Línea 78: Agrega la frase de destino (con tokens) a la lista.\n",
        "    for char in input_text:\n",
        "# Línea 79: Itera sobre los caracteres de la frase de entrada.\n",
        "        if char not in input_characters:\n",
        "# Línea 80: Verifica si el carácter es nuevo.\n",
        "            input_characters.add(char)\n",
        "# Línea 81: Agrega el carácter al conjunto de entrada.\n",
        "    for char in target_text:\n",
        "# Línea 82: Itera sobre los caracteres de la frase de destino.\n",
        "        if char not in target_characters:\n",
        "# Línea 83: Verifica si el carácter es nuevo.\n",
        "            target_characters.add(char)\n",
        "# Línea 84: Agrega el carácter al conjunto de destino.\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "# Línea 86: Convierte y ordena el vocabulario de entrada.\n",
        "target_characters = sorted(list(target_characters))\n",
        "# Línea 87: Convierte y ordena el vocabulario de destino.\n",
        "num_encoder_tokens = len(input_characters)\n",
        "# Línea 88: Calcula el tamaño del vocabulario de entrada.\n",
        "num_decoder_tokens = len(target_characters)\n",
        "# Línea 89: Calcula el tamaño del vocabulario de destino.\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "# Línea 90: Calcula la longitud máxima de la secuencia de entrada.\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "# Línea 91: Calcula la longitud máxima de la secuencia de destino.\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "# Línea 93: Imprime el número de muestras.\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "# Línea 94: Imprime el tamaño del vocabulario de entrada.\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "# Línea 95: Imprime el tamaño del vocabulario de destino.\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "# Línea 96: Imprime la longitud máxima de la secuencia de entrada.\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "# Línea 97: Imprime la longitud máxima de la secuencia de destino.\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "# Línea 99: Crea un diccionario para mapear cada carácter de entrada a su índice numérico.\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "# Línea 100: Crea un diccionario para mapear cada carácter de destino a su índice numérico.\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "# Línea 102: Inicializa el array NumPy para los datos de entrada del Codificador (one-hot).\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "# Línea 103: Forma: (num_muestras, longitud_máx_entrada, tamaño_vocabulario_entrada).\n",
        "    dtype=\"float32\",\n",
        "# Línea 104: Tipo de datos.\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "# Línea 106: Inicializa el array NumPy para los datos de entrada del Decodificador.\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "# Línea 107: Forma: (num_muestras, longitud_máx_destino, tamaño_vocabulario_destino).\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "# Línea 109: Inicializa el array NumPy para los datos de SALIDA/OBJETIVO del Decodificador.\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "# Línea 110: Forma: (num_muestras, longitud_máx_destino, tamaño_vocabulario_destino).\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "# Línea 114: Inicia el bucle de vectorización one-hot.\n",
        "    for t, char in enumerate(input_text):\n",
        "# Línea 115: Itera sobre cada carácter de la frase de entrada.\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "# Línea 116: Codifica en 1.0 la posición del carácter actual en la entrada del codificador.\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "# Línea 117: Rellena el resto de la secuencia de entrada con padding (espacio).\n",
        "    for t, char in enumerate(target_text):\n",
        "# Línea 118: Itera sobre cada carácter de la frase de destino.\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "# Línea 121: Codifica la entrada del decodificador (incluye '\\t').\n",
        "        if t > 0:\n",
        "# Línea 122: Condicional para empezar a codificar el objetivo (salida) un paso después del '\\t'.\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "# Línea 125: Codifica el OBJETIVO del decodificador, desplazado un paso atrás (t-1).\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "# Línea 126: Rellena el resto de la secuencia de entrada del decodificador con padding.\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "# Línea 127: Rellena el resto de la secuencia objetivo del decodificador con padding.\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "# Línea 135: Define la capa de entrada del Codificador.\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "# Línea 136: Define la capa LSTM del Codificador, configurada para devolver solo los estados internos.\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# Línea 137: Conecta la entrada a la LSTM y captura los estados h y c.\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "# Línea 140: Almacena los estados internos del Codificador (vector de contexto).\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "# Línea 143: Define la capa de entrada del Decodificador.\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "# Línea 148: Define la capa LSTM del Decodificador, devuelve secuencias completas y estados.\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "# Línea 149: Conecta la entrada y usa los estados del Codificador como estado inicial.\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "# Línea 150: Define la capa Densa de salida con activación softmax.\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# Línea 151: Conecta la salida de la LSTM a la capa Densa.\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# Línea 155: Crea el Modelo de Entrenamiento completo.\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "model.compile(\n",
        "# Línea 160: Comienza la fase de compilación.\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "# Línea 161: Especifica el optimizador, la función de pérdida y la métrica.\n",
        ")\n",
        "# Nota: La ejecución de model.fit() se omite aquí para fines de compilación del archivo.\n",
        "# model.fit(\n",
        "#     [encoder_input_data, decoder_input_data],\n",
        "#     decoder_target_data,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     validation_split=0.2,\n",
        "# )\n",
        "# Save model\n",
        "model.save(\"s2s_model.keras\")\n",
        "# Línea 171: Guarda el modelo entrenado en el disco.\n",
        "\n",
        "\"\"\"\n",
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n",
        "\"\"\"\n",
        "\n",
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "# Línea 184: Carga el modelo guardado.\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "# Línea 186: Define la entrada del Codificador de inferencia.\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "# Línea 187: Extrae los estados finales de la LSTM del Codificador.\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "# Línea 188: Almacena los estados.\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# Línea 189: Define el MODELO CODIFICADOR DE INFERENCIA.\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "# Línea 191: Define la entrada del Decodificador de inferencia.\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "# Línea 192: Define el placeholder para el estado oculto (h) de entrada.\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "# Línea 193: Define el placeholder para el estado de celda (c) de entrada.\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# Línea 194: Agrupa los estados de entrada del decodificador.\n",
        "decoder_lstm = model.layers[3]\n",
        "# Línea 195: Referencia a la capa LSTM del Decodificador.\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "# Línea 196: Conecta la entrada de la secuencia de 1 paso a la LSTM.\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        "# Línea 197: Usa los placeholders de estados como el estado inicial.\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "# Línea 199: Los estados de salida de la LSTM (los estados actualizados).\n",
        "decoder_dense = model.layers[4]\n",
        "# Línea 200: Referencia a la capa Densa de salida.\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# Línea 201: Conecta la salida de la LSTM a la capa Densa.\n",
        "decoder_model = Model(\n",
        "# Línea 202: Define el MODELO DECODIFICADOR DE INFERENCIA.\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "# Línea 203: Mapea la entrada (1 carácter + estados previos) a (1 carácter predicho + nuevos estados).\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# Línea 209: Crea el diccionario inverso para mapear índices a caracteres de entrada.\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "# Línea 210: Crea el diccionario inverso para mapear índices a caracteres de destino.\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "# Línea 213: Define la función que ejecuta la traducción completa.\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "# Línea 215: Usa el Codificador para obtener el vector de contexto (estados iniciales).\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "# Línea 218: Inicializa la secuencia de entrada del decodificador (tamaño 1).\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "# Línea 220: Inserta el token de inicio de secuencia ('\\t').\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "# Línea 223: Bandera de parada.\n",
        "    decoded_sentence = \"\"\n",
        "# Línea 224: Inicializa la cadena de texto de la traducción.\n",
        "    while not stop_condition:\n",
        "# Línea 225: Bucle de generación (un paso por carácter).\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "# Línea 226: Usa el Decodificador para predecir el siguiente carácter.\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "# Línea 231: Encuentra el índice del carácter con la probabilidad más alta (Greedy search).\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "# Línea 232: Convierte el índice en el carácter real.\n",
        "        decoded_sentence += sampled_char\n",
        "# Línea 233: Agrega el carácter a la frase traducida.\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "# Línea 238: Condición de parada.\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "# Línea 241: Reinicia la secuencia de entrada del decodificador.\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "# Línea 242: El carácter predicho se convierte en la entrada para el siguiente paso.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "# Línea 245: Actualiza los estados para el siguiente paso.\n",
        "    return decoded_sentence\n",
        "# Línea 246: Devuelve la frase traducida.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "You can now generate decoded sentences as such:\n",
        "\"\"\"\n",
        "\n",
        "for seq_index in range(20):\n",
        "# Línea 252: Itera sobre las primeras 20 muestras.\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "# Línea 256: Extrae una sola secuencia de entrada.\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "# Línea 257: Llama a la función de decodificación.\n",
        "    print(\"-\")\n",
        "# Línea 258: Separador visual.\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "# Línea 259: Imprime la frase original.\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n",
        "# Línea 260: Imprime la traducción generada.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. SUBTITULADO DE IMÁGENES (Tarea Avanzada: Reescritura del modelo PyTorch en Keras)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Concatenate, Lambda\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Hyperparámetros basados en el archivo PyTorch model.py ---\n",
        "EMBED_SIZE_IC = 256    # embed_size\n",
        "HIDDEN_SIZE_IC = 512   # hidden_size\n",
        "VOCAB_SIZE_IC = VOCAB_SIZE # Usamos el mismo vocabulario, pero debe ser el de captions.\n",
        "RESNET_FC_IN_FEATURES = 2048 # Salida de ResNet152 antes del mapeo.\n",
        "\n",
        "# =================================================================\n",
        "# CODIFICADOR CNN (Simula EncoderCNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_encoder_ic():\n",
        "    \"\"\"Toma la imagen y extrae el vector de características.\"\"\"\n",
        "\n",
        "    # Keras espera (Alto, Ancho, Canales) para la imagen\n",
        "    image_input = Input(shape=(224, 224, 3), name='ic_image_input')\n",
        "\n",
        "    # ResNet152 sin la capa densa superior, con Global Average Pooling\n",
        "    resnet = ResNet152(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "    # Extraer características de la CNN (shape=(None, 2048))\n",
        "    cnn_features = resnet(image_input)\n",
        "\n",
        "    # Capa Lineal (simula self.linear): mapea 2048 a EMBED_SIZE_IC (256)\n",
        "    encoder_output = Dense(EMBED_SIZE_IC, activation='relu', name='ic_encoder_linear')(cnn_features)\n",
        "\n",
        "    # Se podría añadir una capa BatchNormalization aquí para simular self.bn\n",
        "\n",
        "    encoder_model = Model(inputs=image_input, outputs=encoder_output, name='IC_Encoder_CNN')\n",
        "\n",
        "    return encoder_model, image_input, encoder_output\n",
        "\n",
        "# =================================================================\n",
        "# DECODIFICADOR RNN (Simula DecoderRNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_decoder_ic(feature_input_tensor):\n",
        "    \"\"\"Genera la descripción a partir de las características y la secuencia.\"\"\"\n",
        "\n",
        "    # 1. Entrada de la secuencia de palabras (captions)\n",
        "    caption_inputs = Input(shape=(None,), name='ic_caption_input')\n",
        "\n",
        "    # 2. Capa de Embedding (simula self.embed)\n",
        "    embedding_layer = Embedding(VOCAB_SIZE_IC, EMBED_SIZE_IC, name='ic_decoder_embedding')\n",
        "    embeddings = embedding_layer(caption_inputs)\n",
        "\n",
        "    # 3. Concatenación (Simula torch.cat((features.unsqueeze(1), embeddings), 1))\n",
        "    # La característica de la imagen actúa como el primer token/paso de tiempo.\n",
        "\n",
        "    # Agrega la dimensión de tiempo a la característica de la imagen (Ej. de (256,) a (1, 256))\n",
        "    feature_step = Lambda(lambda x: K.expand_dims(x, axis=1), name='ic_feature_step')(feature_input_tensor)\n",
        "\n",
        "    # Concatena: [Feature_Image (Paso 0)] + [Embedded_Caption (Pasos 1 a N)]\n",
        "    combined_input = Concatenate(axis=1, name='ic_combined_input')([feature_step, embeddings])\n",
        "\n",
        "    # 4. Capa LSTM (simula self.lstm)\n",
        "    # batch_first=True es el default en Keras.\n",
        "    lstm_layer = LSTM(HIDDEN_SIZE_IC, return_sequences=True, name='ic_decoder_lstm')\n",
        "    hiddens = lstm_layer(combined_input)\n",
        "\n",
        "    # 5. Capa Lineal (simula self.linear)\n",
        "    linear_output = Dense(VOCAB_SIZE_IC, activation='softmax', name='ic_decoder_output')\n",
        "    outputs = linear_output(hiddens)\n",
        "\n",
        "    return caption_inputs, outputs\n",
        "\n",
        "# =================================================================\n",
        "# ENSAMBLAJE DEL MODELO COMPLETO DE SUBTITULADO\n",
        "# =================================================================\n",
        "\n",
        "def create_full_captioning_model():\n",
        "    \"\"\"Ensambla el Codificador y el Decodificador.\"\"\"\n",
        "\n",
        "    encoder_model, image_input_tensor, feature_output_tensor = build_keras_encoder_ic()\n",
        "    caption_input_tensor, decoder_output_tensor = build_keras_decoder_ic(feature_output_tensor)\n",
        "\n",
        "    full_model = Model(\n",
        "        inputs=[image_input_tensor, caption_input_tensor],\n",
        "        outputs=decoder_output_tensor,\n",
        "        name='Image_Captioning_Seq2Seq'\n",
        "    )\n",
        "\n",
        "    # Compilación (requerida para el entrenamiento)\n",
        "    full_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # return full_model # Retorna el modelo de entrenamiento\n",
        "    print(\"\\n--- TAREA AVANZADA: MODELO DE SUBTITULADO DE IMÁGENES EN KERAS ---\")\n",
        "    print(\"Modelo completo de subtitulado de imágenes (simulación PyTorch model.py) creado.\")\n",
        "    full_model.summary()\n",
        "\n",
        "# Ejecución para mostrar la arquitectura:\n",
        "if __name__ == '__main__':\n",
        "    # Esto ejecutaría la traducción automática (si model.fit no estuviera comentado)\n",
        "    # Y luego mostraría el resumen del modelo de subtitulado.\n",
        "    create_full_captioning_model()"
      ],
      "metadata": {
        "id": "mjRy4U4zrr9h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================================\n",
        "3.2 INVESTIGACIÓN: EJECUCIÓN DEL MODELO DE SUBTITULADO EN KERAS Y MIGRACIÓN DE PESOS PYTORCH\n",
        "================================================================================================\n",
        "\n",
        "https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\n",
        "\n",
        "Este bloque describe los pasos conceptuales necesarios para migrar un modelo entrenado en\n",
        "PyTorch (como el de subtitulado de imágenes) para ser ejecutado y utilizado en Keras/TensorFlow.\n",
        "\n",
        "--- PASOS CLAVE PARA LA MIGRACIÓN ---\n",
        "\n",
        "1. DEFINIR LA ARQUITECTURA EQUIVALENTE EN KERAS:\n",
        "   Se debe replicar la estructura del modelo de PyTorch (EncoderCNN + DecoderRNN)\n",
        "   en Keras utilizando el Modelo Funcional. Las capas LSTM, Dense y Embedding de Keras\n",
        "   deben tener exactamente las mismas dimensiones y configuraciones que las capas de PyTorch.\n",
        "\n",
        "2. CARGA DE PESOS DE PYTORCH:\n",
        "   Se utiliza PyTorch (torch.load) para cargar el archivo de pesos (.pth o .pt).\n",
        "   Esto resulta en un diccionario de estados (state_dict) que contiene los tensores\n",
        "   de peso por nombre de capa (ej., 'lstm.weight_ih_l0').\n",
        "\n",
        "3. MAPEADO Y TRANSPOSICIÓN DE TENSORES (Paso Crítico):\n",
        "   Keras/TensorFlow y PyTorch tienen convenciones diferentes para almacenar los pesos.\n",
        "\n",
        "   A. CAPAS LINEALES/DENSAS (nn.Linear vs. keras.layers.Dense):\n",
        "      - PyTorch almacena los pesos en el formato: [output_dim, input_dim].\n",
        "      - Keras/TensorFlow almacena los pesos en el formato: [input_dim, output_dim].\n",
        "      - ACCIÓN REQUERIDA: Los tensores de peso de PyTorch deben ser **TRANSPUESTOS** (usando .T o np.transpose)\n",
        "        antes de ser asignados a la capa Densa de Keras.\n",
        "\n",
        "        # Ejemplo conceptual:\n",
        "        # keras_weights = pytorch_weights.transpose()\n",
        "        # keras_biases = pytorch_biases (Los sesgos generalmente no necesitan transposición)\n",
        "\n",
        "   B. CAPAS LSTM/GRU (nn.LSTM vs. keras.layers.LSTM):\n",
        "      - Los tensores de LSTM (pesos de entrada, pesos recurrentes, sesgos) están concatenados\n",
        "        de forma diferente en cada framework.\n",
        "      - ACCIÓN REQUERIDA: Los tensores de PyTorch deben ser **divididos, reordenados y luego transpuestos**\n",
        "        para que coincidan con el orden esperado por Keras: [kernel, recurrent_kernel, bias].\n",
        "\n",
        "4. ASIGNACIÓN DE PESOS EN KERAS:\n",
        "   Una vez que los tensores han sido convertidos y reestructurados, se inyectan en las\n",
        "   capas correspondientes de Keras usando el método:\n",
        "\n",
        "   # Ejemplo:\n",
        "   # keras_layer = model.get_layer('nombre_de_la_capa_keras')\n",
        "   # keras_layer.set_weights([converted_weights, converted_biases])\n",
        "\n",
        "Este proceso garantiza que el modelo Keras utilice la inteligencia aprendida por el modelo PyTorch.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "u6BM-Hvw1zAL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Lambda\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Hyperparámetros Basados en el modelo PyTorch ---\n",
        "EMBED_SIZE = 256    # Dimensión del embedding\n",
        "HIDDEN_SIZE = 512   # Unidades de la LSTM\n",
        "VOCAB_SIZE = 10000  # Tamaño del vocabulario (ejemplo, ajustar al real)\n",
        "\n",
        "# =================================================================\n",
        "# 1. CODIFICADOR CNN (Simula EncoderCNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_encoder_ic():\n",
        "    \"\"\"Define el Codificador que extrae el vector de características de la imagen.\"\"\"\n",
        "\n",
        "    # Entrada de la imagen (Ej. 224x224x3)\n",
        "    image_input = Input(shape=(224, 224, 3), name='ic_image_input')\n",
        "\n",
        "    # ResNet152 preentrenada, quitando la capa densa superior, usando Global Average Pooling\n",
        "    resnet = ResNet152(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "    # Características de la CNN (Salida de pooling: 2048)\n",
        "    cnn_features = resnet(image_input)\n",
        "\n",
        "    # Capa Lineal (simula self.linear): Mapea 2048 características a EMBED_SIZE (256)\n",
        "    # Este vector se usará como el primer token para el Decodificador.\n",
        "    encoder_output = Dense(EMBED_SIZE, activation='relu', name='ic_encoder_linear')(cnn_features)\n",
        "\n",
        "    return image_input, encoder_output\n",
        "\n",
        "# =================================================================\n",
        "# 2. DECODIFICADOR RNN (Simula DecoderRNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_decoder_ic(feature_input_tensor):\n",
        "    \"\"\"Define el Decodificador que genera la descripción secuencialmente.\"\"\"\n",
        "\n",
        "    # 1. Entrada de la secuencia de palabras (captions)\n",
        "    caption_inputs = Input(shape=(None,), name='ic_caption_input')\n",
        "\n",
        "    # 2. Capa de Embedding (simula self.embed)\n",
        "    embedding_layer = Embedding(VOCAB_SIZE, EMBED_SIZE, mask_zero=True, name='ic_decoder_embedding')\n",
        "    embeddings = embedding_layer(caption_inputs)\n",
        "\n",
        "    # 3. Concatenación de Feature de Imagen como el primer token\n",
        "    # Agrega la dimensión de tiempo a la característica de la imagen (de (256,) a (1, 256))\n",
        "    feature_step = Lambda(lambda x: K.expand_dims(x, axis=1), name='ic_feature_step')(feature_input_tensor)\n",
        "\n",
        "    # Concatena: [Feature_Image (Paso 0)] + [Embedded_Caption (Pasos 1 a N)]\n",
        "    combined_input = Concatenate(axis=1, name='ic_combined_input')([feature_step, embeddings])\n",
        "\n",
        "    # 4. Capa LSTM (simula self.lstm)\n",
        "    # return_sequences=True para predecir un token en cada paso.\n",
        "    lstm_layer = LSTM(HIDDEN_SIZE, return_sequences=True, name='ic_decoder_lstm')\n",
        "    hiddens = lstm_layer(combined_input)\n",
        "\n",
        "    # 5. Capa Lineal (simula self.linear)\n",
        "    decoder_outputs = Dense(VOCAB_SIZE, activation='softmax', name='ic_decoder_output')(hiddens)\n",
        "\n",
        "    return caption_inputs, decoder_outputs\n",
        "\n",
        "# =================================================================\n",
        "# 3. MODELO DE SUBTITULADO COMPLETO (Entrenamiento)\n",
        "# =================================================================\n",
        "\n",
        "image_input_tensor, feature_output_tensor = build_keras_encoder_ic()\n",
        "caption_input_tensor, decoder_output_tensor = build_keras_decoder_ic(feature_output_tensor)\n",
        "\n",
        "# Modelo completo: [Imagen, Caption_Input] -> [Caption_Output]\n",
        "captioning_model = Model(\n",
        "    inputs=[image_input_tensor, caption_input_tensor],\n",
        "    outputs=decoder_output_tensor,\n",
        "    name='Image_Captioning_Keras_Simulation'\n",
        ")\n",
        "\n",
        "# Ejemplo de Compilación:\n",
        "# captioning_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# captioning_model.summary()"
      ],
      "metadata": {
        "id": "K0xOQ-ewxObt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================================\n",
        "4. TAREA AVANZADA: INVESTIGACIÓN AVANZADA\n",
        "================================================================================================\n",
        "\n",
        "Este bloque contiene las respuestas a las preguntas de investigación avanzada,\n",
        "estructuradas como documentación dentro del archivo Python.\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.1. Traducción entre japonés e inglés (Jp <-> En)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "PREGUNTA: ¿Qué pasos se tomarían para traducir entre japonés e inglés?\n",
        "\n",
        "El japonés (SOV: Sujeto-Objeto-Verbo) y el inglés (SVO: Sujeto-Verbo-Objeto)\n",
        "son tipológicamente muy diferentes, lo que requiere métodos avanzados de tokenización\n",
        "y reordenamiento.\n",
        "\"\"\"\n",
        "\n",
        "def pasos_traduccion_jp_en():\n",
        "    \"\"\"Describe los pasos clave para la traducción Japonés <-> Inglés.\"\"\"\n",
        "\n",
        "    # 1. Tokenización (Paso Crítico)\n",
        "    print(\"1. Tokenización por Sub-Palabra:\")\n",
        "    # El japonés no usa espacios, por lo que la tokenización debe ser morfológica.\n",
        "    # Se usarían algoritmos como SentencePiece o WordPiece para segmentar el texto\n",
        "    # japonés en unidades (sub-palabras) que manejen los diferentes alfabetos (Kanji, Hiragana, Katakana).\n",
        "\n",
        "    # 2. Reordenamiento Sintáctico\n",
        "    print(\"2. Reordenamiento Sintáctico (SOV <-> SVO):\")\n",
        "    # Se requiere un modelo avanzado (Transformer o NMT con Atención) capaz de aprender\n",
        "    # las complejas reglas para reordenar la estructura de la frase (por ejemplo, mover\n",
        "    # el verbo al final en japonés y al centro en inglés).\n",
        "\n",
        "    # 3. Corpus\n",
        "    print(\"3. Uso de Corpus Paralelo Extenso:\")\n",
        "    # El entrenamiento requiere un corpus paralelo de gran calidad y tamaño (ej. ASPEC)\n",
        "    # para que el modelo aprenda las reglas de mapeo de alta complejidad.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.2. Métodos avanzados de traducción automática\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def metodos_avanzados_traduccion():\n",
        "    \"\"\"Explora los métodos más allá del Seq2Seq básico.\"\"\"\n",
        "\n",
        "    # A. Traducción Automática Neuronal basada en Atención (NMT con Attention)\n",
        "    print(\"\\nA. NMT basado en Atención:\")\n",
        "    # Mecanismo: El decodificador calcula un vector de atención, un promedio ponderado\n",
        "    # de TODOS los estados ocultos del codificador.\n",
        "    # Ventaja: Permite al modelo 'enfocarse' dinámicamente en las partes relevantes\n",
        "    # de la frase de origen mientras traduce cada palabra de salida. Esto resuelve\n",
        "    # el 'cuello de botella' del Seq2Seq básico.\n",
        "\n",
        "    # B. Modelos Transformer (El Estado del Arte)\n",
        "    print(\"B. Modelos Transformer:\")\n",
        "    # Arquitectura: Abandonan las RNNs (LSTM/GRU) por completo. Se basan únicamente\n",
        "    # en el mecanismo de Auto-Atención (Self-Attention) y Atención Multi-Head.\n",
        "    # Ventaja: Permite el procesamiento en paralelo de la secuencia completa,\n",
        "    # acelerando dramáticamente el entrenamiento y capturando mejor las dependencias\n",
        "    # a larga distancia.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.3. Generación de imágenes a partir de texto\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def generacion_imagenes_texto():\n",
        "    \"\"\"Investiga la tecnología inversa al subtitulado de imágenes.\"\"\"\n",
        "\n",
        "    # El campo es dominado por los Modelos de Difusión.\n",
        "\n",
        "    # 1. Modelos de Difusión (Diffusion Models)\n",
        "    print(\"\\n1. Modelos de Difusión (Estado del Arte):\")\n",
        "    # Mecanismo: El modelo se entrena para revertir un proceso progresivo de\n",
        "    # adición de ruido (denoising).\n",
        "    # Condicionamiento: El prompt de texto se inyecta (codificado, ej., usando CLIP)\n",
        "    # como una condición en cada paso del proceso de 'denoising', guiando la\n",
        "    # reconstrucción de la imagen para que coincida con la descripción.\n",
        "    # Ejemplos: Stable Diffusion, DALL-E 2.\n",
        "\n",
        "    # 2. Redes Generativas Adversarias Condicionadas (GANs)\n",
        "    print(\"2. GANs Condicionadas:\")\n",
        "    # Mecanismo: Dos redes compiten. La Generadora crea la imagen a partir del texto,\n",
        "    # y la Discriminadora juzga si la imagen es realista y si coincide con la descripción textual.\n",
        "\n",
        "# =================================================================\n",
        "# EJECUCIÓN (Llamar a las funciones para ver la documentación)\n",
        "# =================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"--- 4.1. Traducción Japonés <-> Inglés ---\")\n",
        "    pasos_traduccion_jp_en()\n",
        "\n",
        "    print(\"\\n--- 4.2. Métodos Avanzados de Traducción Automática ---\")\n",
        "    metodos_avanzados_traduccion()\n",
        "\n",
        "    print(\"\\n--- 4.3. Generación de Imágenes a partir de Texto ---\")\n",
        "    generacion_imagenes_texto()"
      ],
      "metadata": {
        "id": "pSQAFjiB0dD_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HmC-Q4w3SSB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}