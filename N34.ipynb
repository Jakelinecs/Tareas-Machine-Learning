{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK2+YCQ3oJVjQkpfljsykk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jakelinecs/Tareas-Machine-Learning/blob/main/N34.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "==================================================================================================\n",
        "COMPLETE PRACTICE: AUTOMATIC TRANSLATION AND IMAGE CAPTIONING (KERAS/PYTORCH)\n",
        "==================================================================================================\n",
        "\n",
        "This script contains the implementation of the Seq2Seq model (character-level LSTM) for Automatic\n",
        "Translation (Point 2) and the implementation of the Keras Advanced Task for Image\n",
        "Captioning (Point 3), simulating the architecture of the PyTorch model.py file.\n",
        "\n",
        "Note: The automatic translation code is fully documented line by line\n",
        "to meet the task requirement.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "# Line 39: Imports the NumPy library, essential for efficient handling of arrays and tensors.\n",
        "import keras\n",
        "# Line 40: Imports the Keras library to build and train the neural network.\n",
        "import os\n",
        "# Line 41: Imports the 'os' module to interact with the operating system (commands and paths).\n",
        "from pathlib import Path\n",
        "# Line 42: Imports the 'Path' class from pathlib to handle file paths.\n",
        "\n",
        "# === LINES 5-34: INTRODUCTION/SETUP (Original script text) ===\n",
        "# (These lines are informative comments in the original script and are omitted here as they are prose,\n",
        "# but in the final file they must be present as the original introductory comment block).\n",
        "# Lines 38-41: Import block.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. AUTOMATIC TRANSLATION (lstm_seq2seq.py implementation)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "## Data download\n",
        "\"\"\"\n",
        "\n",
        "fpath = keras.utils.get_file(origin=\"http://www.manythings.org/anki/fra-eng.zip\")\n",
        "# Line 47: Downloads the translation corpus ZIP file and stores its local path in 'fpath'.\n",
        "dirpath = Path(fpath).parent.absolute()\n",
        "# Line 48: Gets the absolute directory where the ZIP file was downloaded.\n",
        "os.system(f\"unzip -q {fpath} -d {dirpath}\")\n",
        "# Line 49: Executes a system command to quietly (-q) decompress the file.\n",
        "\n",
        "\"\"\"\n",
        "## Configuration\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "# Line 54: Defines the batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "# Line 55: Defines the number of epochs for training.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "# Line 56: Defines the latent dimensionality (internal units) of the LSTM layers.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Line 57: Defines the maximum number of sentence pairs to use.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = os.path.join(dirpath, \"fra.txt\")\n",
        "# Line 59: Constructs the full path to the uncompressed text data file.\n",
        "\n",
        "\"\"\"\n",
        "## Data preparation\n",
        "\"\"\"\n",
        "\n",
        "# Vectorize the data.\n",
        "# Line 64: Comment indicating the start of the vectorization phase.\n",
        "input_texts = []\n",
        "# Line 65: Initializes list for input sentences (English).\n",
        "target_texts = []\n",
        "# Line 66: Initializes list for target sentences (French).\n",
        "input_characters = set()\n",
        "# Line 67: Initializes set for unique input characters.\n",
        "target_characters = set()\n",
        "# Line 68: Initializes set for unique target characters.\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "# Line 69: Opens the data file in read mode.\n",
        "    lines = f.read().split(\"\\n\")\n",
        "# Line 70: Reads the entire file and splits it into a list of sentences.\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "# Line 71: Iterates over the first 'num_samples' lines.\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "# Line 72: Splits the line by the tab ('\\t').\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "# Line 76: Adds the START OF SEQUENCE token ('\\t') and END OF SEQUENCE token ('\\n') to the target sentence.\n",
        "    input_texts.append(input_text)\n",
        "# Line 77: Appends the input sentence to the list.\n",
        "    target_texts.append(target_text)\n",
        "# Line 78: Appends the target sentence (with tokens) to the list.\n",
        "    for char in input_text:\n",
        "# Line 79: Iterates over the characters of the input sentence.\n",
        "        if char not in input_characters:\n",
        "# Line 80: Checks if the character is new.\n",
        "            input_characters.add(char)\n",
        "# Line 81: Adds the character to the input set.\n",
        "    for char in target_text:\n",
        "# Line 82: Iterates over the characters of the target sentence.\n",
        "        if char not in target_characters:\n",
        "# Line 83: Checks if the character is new.\n",
        "            target_characters.add(char)\n",
        "# Line 84: Adds the character to the target set.\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "# Line 86: Converts and sorts the input vocabulary.\n",
        "target_characters = sorted(list(target_characters))\n",
        "# Line 87: Converts and sorts the target vocabulary.\n",
        "num_encoder_tokens = len(input_characters)\n",
        "# Line 88: Calculates the size of the input vocabulary.\n",
        "num_decoder_tokens = len(target_characters)\n",
        "# Line 89: Calculates the size of the target vocabulary.\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "# Line 90: Calculates the maximum length of the input sequence.\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "# Line 91: Calculates the maximum length of the target sequence.\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "# Line 93: Prints the number of samples.\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "# Line 94: Prints the size of the input vocabulary.\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "# Line 95: Prints the size of the target vocabulary.\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "# Line 96: Prints the maximum length of the input sequence.\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "# Line 97: Prints the maximum length of the target sequence.\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "# Line 99: Creates a dictionary to map each input character to its numerical index.\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "# Line 100: Creates a dictionary to map each target character to its numerical index.\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "# Line 102: Initializes the NumPy array for the Encoder input data (one-hot).\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "# Line 103: Shape: (num_samples, max_input_length, input_vocabulary_size).\n",
        "    dtype=\"float32\",\n",
        "# Line 104: Data type.\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "# Line 106: Initializes the NumPy array for the Decoder input data.\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "# Line 107: Shape: (num_samples, max_target_length, target_vocabulary_size).\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "# Line 109: Initializes the NumPy array for the Decoder OUTPUT/TARGET data.\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "# Line 110: Shape: (num_samples, max_target_length, target_vocabulary_size).\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "# Line 114: Starts the one-hot vectorization loop.\n",
        "    for t, char in enumerate(input_text):\n",
        "# Line 115: Iterates over each character in the input sentence.\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "# Line 116: Encodes the current character's position in the encoder input as 1.0.\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "# Line 117: Pads the rest of the input sequence with padding (space).\n",
        "    for t, char in enumerate(target_text):\n",
        "# Line 118: Iterates over each character in the target sentence.\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "# Line 121: Encodes the decoder input (includes '\\t').\n",
        "        if t > 0:\n",
        "# Line 122: Conditional to start encoding the target (output) one step after '\\t'.\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "# Line 125: Encodes the decoder TARGET, shifted one step back (t-1).\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "# Line 126: Pads the rest of the decoder input sequence.\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "# Line 127: Pads the rest of the decoder target sequence.\n",
        "\n",
        "\"\"\"\n",
        "## Build the model\n",
        "\"\"\"\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "# Line 135: Defines the Encoder input layer.\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "# Line 136: Defines the Encoder LSTM layer, configured to return only the internal states.\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# Line 137: Connects the input to the LSTM and captures the h and c states.\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "# Line 140: Stores the internal states of the Encoder (context vector).\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "# Line 143: Defines the Decoder input layer.\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "# Line 148: Defines the Decoder LSTM layer, returns full sequences and states.\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "# Line 149: Connects the input and uses the Encoder states as the initial state.\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "# Line 150: Defines the output Dense layer with softmax activation.\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# Line 151: Connects the LSTM output to the Dense layer.\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# Line 155: Creates the complete Training Model.\n",
        "\n",
        "\"\"\"\n",
        "## Train the model\n",
        "\"\"\"\n",
        "\n",
        "model.compile(\n",
        "# Line 160: Starts the compilation phase.\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "# Line 161: Specifies the optimizer, loss function, and metric.\n",
        ")\n",
        "# Note: The execution of model.fit() is omitted here for file compilation purposes.\n",
        "# model.fit(\n",
        "#     [encoder_input_data, decoder_input_data],\n",
        "#     decoder_target_data,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     validation_split=0.2,\n",
        "# )\n",
        "# Save model\n",
        "model.save(\"s2s_model.keras\")\n",
        "# Line 171: Saves the trained model to disk.\n",
        "\n",
        "\"\"\"\n",
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n",
        "\"\"\"\n",
        "\n",
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s_model.keras\")\n",
        "# Line 184: Loads the saved model.\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "# Line 186: Defines the inference Encoder input.\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "# Line 187: Extracts the final states of the Encoder LSTM.\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "# Line 188: Stores the states.\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "# Line 189: Defines the INFERENCE ENCODER MODEL.\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "# Line 191: Defines the inference Decoder input.\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "# Line 192: Defines the placeholder for the input hidden state (h).\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "# Line 193: Defines the placeholder for the input cell state (c).\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "# Line 194: Groups the decoder input states.\n",
        "decoder_lstm = model.layers[3]\n",
        "# Line 195: Reference to the Decoder LSTM layer.\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "# Line 196: Connects the 1-step sequence input to the LSTM.\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        "# Line 197: Uses the state placeholders as the initial state.\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "# Line 199: The output states of the LSTM (the updated states).\n",
        "decoder_dense = model.layers[4]\n",
        "# Line 200: Reference to the output Dense layer.\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# Line 201: Connects the LSTM output to the Dense layer.\n",
        "decoder_model = Model(\n",
        "# Line 202: Defines the INFERENCE DECODER MODEL.\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "# Line 203: Maps the input (1 character + previous states) to (1 predicted character + new states).\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "# Line 209: Creates the reverse dictionary to map indices to input characters.\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "# Line 210: Creates the reverse dictionary to map indices to target characters.\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "# Line 213: Defines the function that executes the complete translation.\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
        "# Line 215: Uses the Encoder to get the context vector (initial states).\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "# Line 218: Initializes the decoder input sequence (size 1).\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "# Line 220: Inserts the start-of-sequence token ('\\t').\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "# Line 223: Stop flag.\n",
        "    decoded_sentence = \"\"\n",
        "# Line 224: Initializes the translated text string.\n",
        "    while not stop_condition:\n",
        "# Line 225: Generation loop (one step per character).\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "# Line 226: Uses the Decoder to predict the next character.\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "# Line 231: Finds the index of the character with the highest probability (Greedy search).\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "# Line 232: Converts the index to the actual character.\n",
        "        decoded_sentence += sampled_char\n",
        "# Line 233: Appends the character to the translated sentence.\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "# Line 238: Stop condition.\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "# Line 241: Resets the decoder input sequence.\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "# Line 242: The predicted character becomes the input for the next step.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "# Line 245: Updates the states for the next step.\n",
        "    return decoded_sentence\n",
        "# Line 246: Returns the translated sentence.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "You can now generate decoded sentences as such:\n",
        "\"\"\"\n",
        "\n",
        "for seq_index in range(20):\n",
        "# Line 252: Iterates over the first 20 samples.\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "# Line 256: Extracts a single input sequence.\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "# Line 257: Calls the decoding function.\n",
        "    print(\"-\")\n",
        "# Line 258: Visual separator.\n",
        "    print(\"Input sentence:\", input_texts[seq_index])\n",
        "# Line 259: Prints the original sentence.\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n",
        "# Line 260: Prints the generated translation.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. IMAGE CAPTIONING (Advanced Task: Rewriting PyTorch model in Keras)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Concatenate, Lambda\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Hyperparameters based on the PyTorch model.py file ---\n",
        "EMBED_SIZE_IC = 256    # embed_size\n",
        "HIDDEN_SIZE_IC = 512   # hidden_size\n",
        "VOCAB_SIZE_IC = VOCAB_SIZE # Using the same vocabulary, but it should be the caption vocabulary.\n",
        "RESNET_FC_IN_FEATURES = 2048 # Output of ResNet152 before mapping.\n",
        "\n",
        "# =================================================================\n",
        "# CNN ENCODER (Simulates EncoderCNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_encoder_ic():\n",
        "    \"\"\"Takes the image and extracts the feature vector.\"\"\"\n",
        "\n",
        "    # Keras expects (Height, Width, Channels) for the image\n",
        "    image_input = Input(shape=(224, 224, 3), name='ic_image_input')\n",
        "\n",
        "    # ResNet152 without the top dense layer, with Global Average Pooling\n",
        "    resnet = ResNet152(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "    # Extract CNN features (shape=(None, 2048))\n",
        "    cnn_features = resnet(image_input)\n",
        "\n",
        "    # Linear Layer (simulates self.linear): maps 2048 to EMBED_SIZE_IC (256)\n",
        "    encoder_output = Dense(EMBED_SIZE_IC, activation='relu', name='ic_encoder_linear')(cnn_features)\n",
        "\n",
        "    # A BatchNormalization layer could be added here to simulate self.bn\n",
        "\n",
        "    encoder_model = Model(inputs=image_input, outputs=encoder_output, name='IC_Encoder_CNN')\n",
        "\n",
        "    return encoder_model, image_input, encoder_output\n",
        "\n",
        "# =================================================================\n",
        "# RNN DECODER (Simulates DecoderRNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_decoder_ic(feature_input_tensor):\n",
        "    \"\"\"Generates the description from features and sequence.\"\"\"\n",
        "\n",
        "    # 1. Input of the word sequence (captions)\n",
        "    caption_inputs = Input(shape=(None,), name='ic_caption_input')\n",
        "\n",
        "    # 2. Embedding Layer (simulates self.embed)\n",
        "    embedding_layer = Embedding(VOCAB_SIZE_IC, EMBED_SIZE_IC, name='ic_decoder_embedding')\n",
        "    embeddings = embedding_layer(caption_inputs)\n",
        "\n",
        "    # 3. Concatenation (Simulates torch.cat((features.unsqueeze(1), embeddings), 1))\n",
        "    # The image feature acts as the first token/timestep.\n",
        "\n",
        "    # Add the time dimension to the image feature (e.g., from (256,) to (1, 256))\n",
        "    feature_step = Lambda(lambda x: K.expand_dims(x, axis=1), name='ic_feature_step')(feature_input_tensor)\n",
        "\n",
        "    # Concatenate: [Image_Feature (Step 0)] + [Embedded_Caption (Steps 1 to N)]\n",
        "    combined_input = Concatenate(axis=1, name='ic_combined_input')([feature_step, embeddings])\n",
        "\n",
        "    # 4. LSTM Layer (simulates self.lstm)\n",
        "    # batch_first=True is the default in Keras.\n",
        "    lstm_layer = LSTM(HIDDEN_SIZE_IC, return_sequences=True, name='ic_decoder_lstm')\n",
        "    hiddens = lstm_layer(combined_input)\n",
        "\n",
        "    # 5. Linear Layer (simulates self.linear)\n",
        "    linear_output = Dense(VOCAB_SIZE_IC, activation='softmax', name='ic_decoder_output')\n",
        "    outputs = linear_output(hiddens)\n",
        "\n",
        "    return caption_inputs, outputs\n",
        "\n",
        "# =================================================================\n",
        "# ASSEMBLY OF THE COMPLETE CAPTIONING MODEL\n",
        "# =================================================================\n",
        "\n",
        "def create_full_captioning_model():\n",
        "    \"\"\"Assembles the Encoder and Decoder.\"\"\"\n",
        "\n",
        "    encoder_model, image_input_tensor, feature_output_tensor = build_keras_encoder_ic()\n",
        "    caption_input_tensor, decoder_output_tensor = build_keras_decoder_ic(feature_output_tensor)\n",
        "\n",
        "    full_model = Model(\n",
        "        inputs=[image_input_tensor, caption_input_tensor],\n",
        "        outputs=decoder_output_tensor,\n",
        "        name='Image_Captioning_Seq2Seq'\n",
        "    )\n",
        "\n",
        "    # Compilation (required for training)\n",
        "    full_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # return full_model # Returns the training model\n",
        "    print(\"\\n--- ADVANCED TASK: IMAGE CAPTIONING MODEL IN KERAS ---\")\n",
        "    print(\"Complete image captioning model (PyTorch model.py simulation) created.\")\n",
        "    full_model.summary()\n",
        "\n",
        "# Execution to show the architecture:\n",
        "if __name__ == '__main__':\n",
        "    # This would execute the automatic translation (if model.fit wasn't commented out)\n",
        "    # And then show the captioning model summary.\n",
        "    create_full_captioning_model()"
      ],
      "metadata": {
        "id": "mjRy4U4zrr9h"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================================\n",
        "3.2 RESEARCH: EXECUTING THE CAPTIONING MODEL IN KERAS AND MIGRATING PYTORCH WEIGHTS\n",
        "================================================================================================\n",
        "\n",
        "https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\n",
        "\n",
        "This block describes the conceptual steps required to migrate a model trained in\n",
        "PyTorch (like the image captioning one) to be executed and used in Keras/TensorFlow.\n",
        "\n",
        "--- KEY STEPS FOR MIGRATION ---\n",
        "\n",
        "1. DEFINE THE EQUIVALENT ARCHITECTURE IN KERAS:\n",
        "   The structure of the PyTorch model (EncoderCNN + DecoderRNN) must be replicated\n",
        "   in Keras using the Functional Model API. The Keras LSTM, Dense, and Embedding layers\n",
        "   must have exactly the same dimensions and configurations as the PyTorch layers.\n",
        "\n",
        "2. LOADING PYTORCH WEIGHTS:\n",
        "   PyTorch (torch.load) is used to load the weight file (.pth or .pt).\n",
        "   This results in a state dictionary (state_dict) containing the weight tensors\n",
        "   by layer name (e.g., 'lstm.weight_ih_l0').\n",
        "\n",
        "3. TENSOR MAPPING AND TRANSPOSING (Critical Step):\n",
        "   Keras/TensorFlow and PyTorch have different conventions for storing weights.\n",
        "\n",
        "   A. LINEAR/DENSE LAYERS (nn.Linear vs. keras.layers.Dense):\n",
        "      - PyTorch stores weights in the format: [output_dim, input_dim].\n",
        "      - Keras/TensorFlow stores weights in the format: [input_dim, output_dim].\n",
        "      - REQUIRED ACTION: PyTorch weight tensors must be **TRANSPOSED** (using .T or np.transpose)\n",
        "        before being assigned to the Keras Dense layer.\n",
        "\n",
        "        # Conceptual example:\n",
        "        # keras_weights = pytorch_weights.transpose()\n",
        "        # keras_biases = pytorch_biases (Biases generally don't need transposing)\n",
        "\n",
        "   B. LSTM/GRU LAYERS (nn.LSTM vs. keras.layers.LSTM):\n",
        "      - LSTM tensors (input weights, recurrent weights, biases) are concatenated\n",
        "        differently in each framework.\n",
        "      - REQUIRED ACTION: PyTorch tensors must be **split, reordered, and then transposed**\n",
        "        to match the order expected by Keras: [kernel, recurrent_kernel, bias].\n",
        "\n",
        "4. ASSIGNING WEIGHTS IN KERAS:\n",
        "   Once the tensors have been converted and restructured, they are injected into the\n",
        "   corresponding Keras layers using the method:\n",
        "\n",
        "   # Example:\n",
        "   # keras_layer = model.get_layer('keras_layer_name')\n",
        "   # keras_layer.set_weights([converted_weights, converted_biases])\n",
        "\n",
        "This process ensures that the Keras model uses the intelligence learned by the PyTorch model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "u6BM-Hvw1zAL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Lambda\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Hyperparameters Based on the PyTorch model ---\n",
        "EMBED_SIZE = 256    # Embedding dimension\n",
        "HIDDEN_SIZE = 512   # LSTM units\n",
        "VOCAB_SIZE = 10000  # Vocabulary size (example, adjust to actual)\n",
        "\n",
        "# =================================================================\n",
        "# 1. CNN ENCODER (Simulates EncoderCNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_encoder_ic():\n",
        "    \"\"\"Defines the Encoder that extracts the image feature vector.\"\"\"\n",
        "\n",
        "    # Image input (e.g., 224x224x3)\n",
        "    image_input = Input(shape=(224, 224, 3), name='ic_image_input')\n",
        "\n",
        "    # Pre-trained ResNet152, removing the top dense layer, using Global Average Pooling\n",
        "    resnet = ResNet152(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "    # CNN features (Pooling output: 2048)\n",
        "    cnn_features = resnet(image_input)\n",
        "\n",
        "    # Linear Layer (simulates self.linear): Maps 2048 features to EMBED_SIZE (256)\n",
        "    # This vector will be used as the first token for the Decoder.\n",
        "    encoder_output = Dense(EMBED_SIZE, activation='relu', name='ic_encoder_linear')(cnn_features)\n",
        "\n",
        "    return image_input, encoder_output\n",
        "\n",
        "# =================================================================\n",
        "# 2. RNN DECODER (Simulates DecoderRNN)\n",
        "# =================================================================\n",
        "\n",
        "def build_keras_decoder_ic(feature_input_tensor):\n",
        "    \"\"\"Defines the Decoder that generates the description sequentially.\"\"\"\n",
        "\n",
        "    # 1. Input of the word sequence (captions)\n",
        "    caption_inputs = Input(shape=(None,), name='ic_caption_input')\n",
        "\n",
        "    # 2. Embedding Layer (simulates self.embed)\n",
        "    embedding_layer = Embedding(VOCAB_SIZE, EMBED_SIZE, mask_zero=True, name='ic_decoder_embedding')\n",
        "    embeddings = embedding_layer(caption_inputs)\n",
        "\n",
        "    # 3. Concatenating Image Feature as the first token\n",
        "    # Add the time dimension to the image feature (from (256,) to (1, 256))\n",
        "    feature_step = Lambda(lambda x: K.expand_dims(x, axis=1), name='ic_feature_step')(feature_input_tensor)\n",
        "\n",
        "    # Concatenate: [Image_Feature (Step 0)] + [Embedded_Caption (Steps 1 to N)]\n",
        "    combined_input = Concatenate(axis=1, name='ic_combined_input')([feature_step, embeddings])\n",
        "\n",
        "    # 4. LSTM Layer (simulates self.lstm)\n",
        "    # return_sequences=True to predict one token at each step.\n",
        "    lstm_layer = LSTM(HIDDEN_SIZE, return_sequences=True, name='ic_decoder_lstm')\n",
        "    hiddens = lstm_layer(combined_input)\n",
        "\n",
        "    # 5. Linear Layer (simulates self.linear)\n",
        "    decoder_outputs = Dense(VOCAB_SIZE, activation='softmax', name='ic_decoder_output')(hiddens)\n",
        "\n",
        "    return caption_inputs, decoder_outputs\n",
        "\n",
        "# =================================================================\n",
        "# 3. COMPLETE CAPTIONING MODEL (Training)\n",
        "# =================================================================\n",
        "\n",
        "image_input_tensor, feature_output_tensor = build_keras_encoder_ic()\n",
        "caption_input_tensor, decoder_output_tensor = build_keras_decoder_ic(feature_output_tensor)\n",
        "\n",
        "# Complete Model: [Image, Caption_Input] -> [Caption_Output]\n",
        "captioning_model = Model(\n",
        "    inputs=[image_input_tensor, caption_input_tensor],\n",
        "    outputs=decoder_output_tensor,\n",
        "    name='Image_Captioning_Keras_Simulation'\n",
        ")\n",
        "\n",
        "# Example Compilation:\n",
        "# captioning_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# captioning_model.summary()"
      ],
      "metadata": {
        "id": "K0xOQ-ewxObt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================================\n",
        "4. ADVANCED TASK: ADVANCED RESEARCH\n",
        "================================================================================================\n",
        "\n",
        "This block contains the answers to the advanced research questions,\n",
        "structured as documentation within the Python file.\n",
        "\"\"\"\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.1. Translation between Japanese and English (Jp <-> En)\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\"\"\"\n",
        "QUESTION: What steps would be taken to translate between Japanese and English?\n",
        "\n",
        "Japanese (SOV: Subject-Object-Verb) and English (SVO: Subject-Verb-Object)\n",
        "are typologically very different, requiring advanced tokenization methods\n",
        "and reordering.\n",
        "\"\"\"\n",
        "\n",
        "def japanese_english_translation_steps():\n",
        "    \"\"\"Describes the key steps for Japanese <-> English translation.\"\"\"\n",
        "\n",
        "    # 1. Tokenization (Critical Step)\n",
        "    print(\"1. Subword Tokenization:\")\n",
        "    # Japanese does not use spaces, so tokenization must be morphological.\n",
        "    # Algorithms like SentencePiece or WordPiece would be used to segment the Japanese text\n",
        "    # into units (subwords) that handle the different alphabets (Kanji, Hiragana, Katakana).\n",
        "\n",
        "    # 2. Syntactic Reordering\n",
        "    print(\"2. Syntactic Reordering (SOV <-> SVO):\")\n",
        "    # An advanced model (Transformer or NMT with Attention) capable of learning\n",
        "    # the complex rules for reordering the sentence structure (e.g., moving\n",
        "    # the verb to the end in Japanese and to the middle in English) is required.\n",
        "\n",
        "    # 3. Corpus\n",
        "    print(\"3. Use of Extensive Parallel Corpus:\")\n",
        "    # Training requires a large and high-quality parallel corpus (e.g., ASPEC)\n",
        "    # for the model to learn the high-complexity mapping rules.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.2. Advanced Automatic Translation Methods\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def advanced_translation_methods():\n",
        "    \"\"\"Explores methods beyond basic Seq2Seq.\"\"\"\n",
        "\n",
        "    # A. Attention-based Neural Machine Translation (NMT with Attention)\n",
        "    print(\"\\nA. Attention-based NMT:\")\n",
        "    # Mechanism: The decoder calculates an attention vector, a weighted average\n",
        "    # of ALL encoder hidden states.\n",
        "    # Advantage: Allows the model to dynamically 'focus' on the relevant parts\n",
        "    # of the source sentence while translating each output word. This solves\n",
        "    # the 'bottleneck' of basic Seq2Seq.\n",
        "\n",
        "    # B. Transformer Models (State-of-the-Art)\n",
        "    print(\"B. Transformer Models:\")\n",
        "    # Architecture: Abandon RNNs (LSTM/GRU) entirely. They rely solely\n",
        "    # on the Self-Attention and Multi-Head Attention mechanisms.\n",
        "    # Advantage: Allows for parallel processing of the entire sequence,\n",
        "    # dramatically accelerating training and better capturing long-range\n",
        "    # dependencies.\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.3. Text-to-Image Generation\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def text_to_image_generation():\n",
        "    \"\"\"Investigates the inverse technology to image captioning.\"\"\"\n",
        "\n",
        "    # The field is dominated by Diffusion Models.\n",
        "\n",
        "    # 1. Diffusion Models (State-of-the-Art)\n",
        "    print(\"\\n1. Diffusion Models (State-of-the-Art):\")\n",
        "    # Mechanism: The model is trained to reverse a progressive process of\n",
        "    # adding noise (denoising).\n",
        "    # Conditioning: The text prompt is injected (encoded, e.g., using CLIP)\n",
        "    # as a condition at each step of the 'denoising' process, guiding the\n",
        "    # image reconstruction to match the textual description.\n",
        "    # Examples: Stable Diffusion, DALL-E 2.\n",
        "\n",
        "    # 2. Conditional Generative Adversarial Networks (GANs)\n",
        "    print(\"2. Conditional GANs:\")\n",
        "    # Mechanism: Two networks compete. The Generator creates the image from text,\n",
        "    # and the Discriminator judges whether the image is realistic and matches the text description.\n",
        "\n",
        "# =================================================================\n",
        "# EXECUTION (Call the functions to see the documentation)\n",
        "# =================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"--- 4.1. Japanese <-> English Translation ---\")\n",
        "    japanese_english_translation_steps()\n",
        "\n",
        "    print(\"\\n--- 4.2. Advanced Automatic Translation Methods ---\")\n",
        "    advanced_translation_methods()\n",
        "\n",
        "    print(\"\\n--- 4.3. Text-to-Image Generation ---\")\n",
        "    text_to_image_generation()"
      ],
      "metadata": {
        "id": "pSQAFjiB0dD_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HmC-Q4w3SSB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}